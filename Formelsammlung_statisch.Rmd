---
title: "Formelsammlung Statistik"
author: "Lukas Warode"
output: pdf_document
fontsize: 10pt
---

```{r setup, include=FALSE}
library(tidyverse)

theme_set(theme_classic())

knitr::opts_chunk$set(fig.width = 4, 
                      fig.height = 2.5, 
                      fig.align = "center",
                      cache=FALSE,
                      echo=FALSE,
                      warning=FALSE)

modus <- function(var) {
  table(var) %>% 
    sort(decreasing = TRUE) %>% 
    names() %>% 
    .[1] %>% 
    as.numeric()
}
```

# Maße der zentralen Tendenz

## Modus

- Nominales Skalenniveau
- Häufigster Wert

\large 
$$ \large x_{mod} $$ 
\normalsize

## Median

- Ordinales Skalenniveau
- Mittlere Ausprägung bei Anordnung der Variable

Ungerade Anzahl an Fällen (n):
\large
$$ \large \tilde{x} = x_{(\frac{n+1}{2})}$$ 
\normalsize

```{r median I}
set.seed(42)

random_sample <- sample(1:42, 11)

sort(random_sample)

median(random_sample)
```

Gerade Anzahl an Fällen (n):
\large 
$$\large \tilde{x} =\frac{x_{(\frac{n}{2})} + x_{(\frac{n}{2}+1)}}{2}$$
\normalsize

```{r median II}
random_sample <- random_sample[random_sample != max(random_sample)]

sort(random_sample)

median(random_sample)
```

## Arithmetisches Mittel

- Metrisches Skalenniveau
- Summe aller Fälle durch Anzahl der Fälle teilen

\large 
$$\large \bar{x} = \frac{x_{1} + x_{2} + ... + x_{n}}{n} = \frac{1}{n} \sum_{i=i}^{n} x_{i} $$
\normalsize
```{r mean}
random_sample

mean(random_sample)
```

# Verteilungsformen

## Symmetrisch (Normalverteilung)

\large 
$$\large x_{mod} = \tilde{x} = \bar{x} $$
\normalsize
```{r}
set.seed(42)
random_sample_symmetric <- round(rbeta(1000000, 5, 5), 3)

# random_sample_symmetric <- round(rnorm(n = 10000, mean = 0, sd = 0.5), 1)

# random_sample_symmetric <- round(fGarch::rsnorm(10000, mean = 0, sd = 0.5, xi = -10), 1)

random_sample_symmetric %>% 
  as_tibble() %>% 
  ggplot(aes(value)) + 
  geom_density() +
  geom_vline(xintercept = modus(random_sample_symmetric), alpha = 0.25, linetype = 2) +
  annotate("text", label = expression(x[mod]), x = modus(random_sample_symmetric), y = 1.5, size = 5) +
  # geom_vline(xintercept = median(random_sample_symmetric), alpha = 0.25, linetype = 2) +
  annotate("text", label = expression(tilde(x)) , x = modus(random_sample_symmetric), y = 1, size = 5) +
  # geom_vline(xintercept = mean(random_sample_symmetric), alpha = 0.25, linetype = 2) +
  annotate("text", label = expression(bar(x)), x = modus(random_sample_symmetric), y = 0.5, size = 5) 
```

## Linkssteil / Rechtsschief

\large 
$$\large x_{mod} < \tilde{x} < \bar{x} $$
\normalsize
```{r}
set.seed(42)
random_sample_right_skewed <- round(rbeta(1000000, 2, 4.5), 3)

# random_sample_right_skewed <- round(rnorm(n = 10000, mean = 0, sd = 0.5), 1)

# random_sample_right_skewed <- round(fGarch::rsnorm(10000, mean = 0, sd = 0.5, xi = -10), 1)

random_sample_right_skewed %>% 
  as_tibble() %>% 
  ggplot(aes(value)) + 
  geom_density() +
  geom_vline(xintercept = modus(random_sample_right_skewed), alpha = 0.25, linetype = 2) +
  annotate("text", label = expression(x[mod]), x = modus(random_sample_right_skewed), y = 1.5, size = 5) +
  geom_vline(xintercept = median(random_sample_right_skewed), alpha = 0.25, linetype = 2) +
  annotate("text", label = expression(tilde(x)), x = median(random_sample_right_skewed), y = 1, size = 5) +
  geom_vline(xintercept = mean(random_sample_right_skewed), alpha = 0.25, linetype = 2) +
  annotate("text", label = expression(bar(x)), x = mean(random_sample_right_skewed), y = 0.5, size = 5) 
```

## Rechtssteil / Linksschief

\large 
$$\large x_{mod} > \tilde{x} > \bar{x} $$
\normalsize
```{r}
set.seed(42)
random_sample_left_skewed <- round(rbeta(1000000, 4.5, 2), 3)

random_sample_left_skewed %>% 
  as_tibble() %>% 
  ggplot(aes(value)) + 
  geom_density() +
  geom_vline(xintercept = modus(random_sample_left_skewed), alpha = 0.25, linetype = 2) +
  annotate("text", label = expression(x[mod]), x = modus(random_sample_left_skewed), y = 1.5, size = 5) +
  geom_vline(xintercept = median(random_sample_left_skewed), alpha = 0.25, linetype = 2) +
  annotate("text", label = expression(tilde(x)), x = median(random_sample_left_skewed), y = 1, size = 5) +
  geom_vline(xintercept = mean(random_sample_left_skewed), alpha = 0.25, linetype = 2) +
  annotate("text", label = expression(bar(x)), x = mean(random_sample_left_skewed), y = 0.5, size = 5) 
```

# Streuungsmaße

## Spannweite

- Ordinales Skalenniveau
- Differenz zwischen größter und kleinster Ausprägung

\large 
$$\large  R = x_{max} - x_{min} $$
\normalsize

## Interquartilsabstand (IQR)

- Ordinales Skalenniveau
- Intervall der mittleren 50% der Stichprobe

\large 
$$\large  IQR = Q_{0.75} - Q_{0.25} $$
\normalsize

## Variation (Summe der Abweichungsquadrate)

- Metrisches Skalenniveau
- Englisch: *Sum of squares / sum of squared deviations*

\large 
$$\large  SS_{x} = \sum_{i=1}^n (x_{i} - \bar{x})^2 $$
\normalsize

## Varianz 

- Metrisches Skalenniveau
- Standardisierte Variation

\large 
$$\large  Var(x) = s_{x}^2 = \frac{1}{n} \sum_{i=1}^n (x_{i} - \bar{x})^2 $$
\normalsize

## Standardabweichung

- Metrisches Skalenniveau
- Quadratwurzel der Varianz
- Durchschnittliche Abweichung von Werten zum Arithmetischen Mittel

\large 
$$\large  \sigma_{x_{Population}} = \sqrt{\sigma_{x}^2} = \sqrt{\frac{1}{n} \sum_{i=1}^n (x_{i} - \bar{x})^2}$$
$$\large  s_{x_{Stichprobe}} = \sqrt{s_{x}^2} = \sqrt{\frac{1}{n-1} \sum_{i=1}^n (x_{i} - \bar{x})^2} $$
\normalsize

## Variationskoeffizient (Abweichungskoeffizient)

- Metrisches Skalenniveau
- Relatives Streuungsmaß, d.h. nicht abhängig von der Maßeinheit der Variable
- Standardbweichung in Relation zum Arithmetischen Mittel

\large 
$$\large V_{x} = \frac{s_{x}}{\bar{x}} $$
\normalsize

# Standardfehler

- (Durchschnittliche) Abweichung von Stichprobenkennwerten zu Populationskennwerten, z.B. vom Mittelwert

## Standardfehler des Arithmetischen Mittelwertes (*Standard error of the mean*)

- Symbol des Populationsmittelwertes: $\mu$
- Symbol des Stichprobenmittelwertes $\bar{x}$

\large
$$\large \sigma_{\bar{x}_{Population}} = \frac{\sigma}{\sqrt{n}} = \sqrt{\frac{\sigma^2}{n}} $$
$$\large s_{\bar{x}_{Stichprobe}} = \frac{s}{\sqrt{n}} = \sqrt{\frac{s^2}{n}} $$
$$\large \hat{\sigma}_{\bar{x}_{Population, \ geschätzt}} = \frac{s}{\sqrt{n-1}} = \sqrt{\frac{s^2}{n-1}}$$
\normalsize

## Standardfehler des Anteilswertes

- Symbol des Populationsanteilswertes: $\pi$
- Symbol des Stichprobenanteilswertes: $p_{x}$

\large
$$\large \sigma(p_{x})_{Population} = \sqrt{\frac{\pi_{x} \cdot (1-\pi_{x})}{n}} $$
\normalsize

Schätzung der Populationsvarianz: $\pi_{x} \cdot (1-\pi_{x})$ aus der Stichprobenvarianz: $p_{x} \cdot (1-p_{x})$

\large
$$\large \hat{\sigma}(p_{x})_{Population} = \sqrt{\frac{p_{x} \cdot (1-p_{x})}{n}} $$
\normalsize

# Konfidenzintervall

- Generalisierbarkeit von Parametern aus der Stichprobe (auf die Population)
- Geschätzter Intervallbereich, in dem Parameter der Grundgesamtheit mit einer bestimmten Wahrscheinlichkeit liegen

- Z-Standardisierung: $z = \frac{x-\mu}{\sigma}$

## Konfidenzintervall des Populationsmittelwertes ($\mu_{x}$)

- Kleine Stichproben: t-Verteilung
- Große Stichproben: Standardnormalverteilung

Bestimmung der Intervallgrenzen: 

\large
$$\large \bar{x} - \frac{s_{x}}{\sqrt{n-1}} \cdot z_{(1 - \frac{\alpha}{2})} < \mu_{x} < \bar{x} + \frac{s_{x}}{\sqrt{n-1}} \cdot z_{(1 - \frac{\alpha}{2})}$$
\normalsize

## Konfidenzintervall des Populationsanteilswertes ($\pi_{x}$)

- Standardnormalverteilung (wenn Stichprobe ausreichend groß)

Bestimmung der Intervallgrenzen: 

\large
$$\large p_{x} - \sqrt{\frac{p_{x} \cdot (1-p_{x})}{n}} \cdot z_{(1 - \frac{\alpha}{2})} < \pi_{x} < p_{x} + \sqrt{\frac{p_{x} \cdot (1-p_{x})}{n}} \cdot z_{(1 - \frac{\alpha}{2})}$$
\normalsize

# t-Test

## t-Test: Mittelwert

### t-Test für einen Mittelwert

- $H_{0}$: $\mu_{1} = \mu$
- $H_{A}$: $\mu_{1} \neq \mu$

Teststatistik: 

\large
$$\large Z = \frac{\bar{x}-\mu}{\frac{s_{x}}{\sqrt{n-1}}} $$
\normalsize
- Kritische Testwerte bei $z_{\alpha}$ und $z_{(1 -\alpha)}$

### t-Test für 2 Mittelwerte

- $H_{0}$: $\mu_{1} - \mu_{2} = 0$
- $H_{A}$: $\mu_{1} - \mu_{2} \neq 0$

Teststatistik: 

\large
$$\large Z = \frac{\bar{x}_{1} - \bar{x}_{2}}{\sqrt{\frac{s_{x_{1}}^2}{n_{1}-1}-\frac{s_{x_{2}}^2}{n_{2}-1}}} $$
\normalsize
- Kritische Testwerte bei $z_{\frac{\alpha}{2}}$ und $z_{(1 - \frac{\alpha}{2})}$

## t-Test: Populationsanteil

### t-Test für einen Populationsanteil

- $H_{0}$: $\pi_{1} = \pi$
- $H_{A}$: $\pi_{1} \neq \pi$

Teststatistik: 

\large
$$\large Z = \frac{p - \pi}{\sqrt{\frac{\pi \cdot (1-\pi)}{{n}}}}$$
\normalsize
- Kritische Testwerte bei $z_{\alpha}$ und $z_{(1 -\alpha)}$

### t-Test für 2 Populationsanteile

- $H_{0}$: $\pi_{1} - \pi_{2} = 0$
- $H_{A}$: $\pi_{1} - \pi_{2} \neq 0$

Teststatistik: 

\large
$$\large Z = \frac{p_{1} - p_{2}}{\sqrt{\frac{p_1 \cdot (1-p_1)}{n_1} + \frac{p_2 \cdot (1-p_2)}{n_2}}} $$
\normalsize
- Kritische Testwerte bei $z_{\frac{\alpha}{2}}$ und $z_{(1 - \frac{\alpha}{2})}$

# Chi-Quadrat-Unabhängigkeitstest ($\chi^2$)

- Bivariater Test auf stochastische Unabhängigkeit
  - $H_{0}$: Beide Zufallsvariablen sind stochastisch *unabhängig* voneinander
  - $H_{A}$: Beide Zufallsvariablen sind stochastisch *nicht unabhängig* voneinander
  
\large
$$\large \chi^2 = \sum_{i=1}^I \sum_{j=1}^J \frac{(n_{ij} - e_{ij})^2}{e_{ij}} $$
\normalsize

- $i$: *"Zeilen"*
- $j$: *"Spalten"*
- $n_{ij}$: Beobachtete Häufigkeiten
- $e_{ij}$: Erwartete Häufigkeiten
  - $e_{ij} = \frac{n_i \cdot n_j}{n}$
    - $n_i$: Zeilenhäufigkeit
    - $n_j$: Spaltenhäufigkeit
    
Berechnung der Freiheitsgrade (*degrees of freedom*): $df = (I-1) \cdot (J-1)$

## Zusammenhangsmaße auf Basis von $\chi^2$  

\large
$$\large \phi = \sqrt{\frac{\chi^2}{n}} $$
$$\large Cramer`s \ V = \sqrt{\frac{\chi^2}{n \cdot (k-1)}}$$
$$\large Kontingenzkoeffizient \ C =  \sqrt{\frac{\chi^2}{\chi^2 + n}}$$
$$\large C_{korrigiert} = \frac{C}{\sqrt{\frac{k-1}{k}}} $$
\normalsize

- $k = Kleinste \ Zeilenzahl \ oder \ Spaltenzahl$

# F-Test -- Einfaktorielle Varianzanalyse
  
- F-Test testet den Anteil erklärter Varianz an unerklärter Varianz zwischen mehreren Gruppen
- $x_{ij}$: Beobachtung $i$ in der Gruppe $j$
- $\bar{x}_j$: Mittelwert der Gruppe $j$

- Varianz zwischen den Gruppen: $\sum_{j=1}^p n_j (\bar{x}_j - \bar{x})^2$
  - $df_1: j - 1$

- Varianz innerhalb der Gruppen: $\sum_{j=1}^p \sum_{i=1}^{n_j} (x_{ij} - \bar{x}_j)^2$
  - $df_2: n - j$

Teststatistik:

\large
$$\large F = \frac{erklärte \ Varianz}{unerklärte \ Varianz} = \frac{\frac{Varianz \ zwischen \ den \ Gruppen}{df_1}}{\frac{Varianz \ innerhalb \ der \ Gruppen}{df_2}} = \frac{\frac{\sum_{j=1}^p n_j (\bar{x}_j - \bar{x})^2}{j-1}}{\frac{\sum_{j=1}^p \sum_{i=1}^{n_j} (x_{ij} - \bar{x}_j)^2}{n-j}}$$
\normalsize

# Metrische Zusammenhangsmaße

## Kovarianz

- Kovarianz: Gemeinsame Varianz von 2 Variablen
  - $[-\infty, \ \infty]$
  
\large
$$\large Cov(x,y) = \frac{\sum_{i=1}^n(x_i - \bar{x})(y_i - \bar{y})}{n} = \frac{1}{n} \sum_{i=1}^n(x_i - \bar{x})(y_i - \bar{y})$$
\normalsize

## Korrelation (Pearson-Korrelation: $r$)

- Korrelationskoeffizient: Standardisierte Kovarianz
  - $[-1, \ 1]$

\large
$$\large r_{x, y} =  Corr(x,y) = \frac{Cov(x,y)}{\sigma_x \cdot \sigma_y} = \frac{\frac{1}{n} \sum_{i=1}^n(x_i - \bar{x})(y_i - \bar{y})} {\sigma_x \cdot \sigma_y} = \frac{\frac{1}{n}\sum_{i=1}^n(x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\frac{1}{n}\sum_{i=1}^n(x_i - \bar{x})^2 \cdot \frac{1}{n}\sum_{i=1}^n(y_i - \bar{y})^2}}$$
$$\large = \frac{\sum_{i=1}^n(x_i - \bar{x})(y_i - \bar{y})} {\sqrt{\sum_{i=1}^n(x_i - \bar{x})^2 \cdot \sum_{i=1}^n(y_i - \bar{y})^2}}$$
\normalsize

# Lineare Regression

## Einfache lineare Regression

Lineare Modellfunktion: $y = \alpha + \beta x$

- $y$: Zielgröße
- $\alpha$: Konstante ($y$-Achsenabschnitt)
- $\beta$: Steigung

\
Lineare Regressionsgleichung: $y_i = \alpha + \beta x_i + \varepsilon_i$

- Ziel: Finden der geschätzten Werte $\hat{\alpha}$ und $\hat{\beta}$ für die Parameter $\alpha$ und $\beta$, die die Regressionsgleichung am besten nachbilden können (den besten *fit* ermöglichen)
- $x_i$: Beobachtung $i$ der unabhängigen Variable $x$
- $\varepsilon_i$: Fehlerterm (Residuum): Abweichung der empirischen Beobachtung $y_i$ und dem geschätzten Wert der Regressionsgleichung $\hat{y_i}$
- "Geschätzte" Abweichung der Regressionsgerade von der empirischen Beobachtung: $\hat{\varepsilon_i} = y_i - \hat{y_i} = y_i - \alpha - \beta_{x_i}$

\
Ziel der geschätzten Regressiongleichung (Regressionsgerade): Minimierung der Summe der residualen Quadrate $\hat{\varepsilon_i}^2$
- Minimierung von: $\sum_{i=1}^n \hat{\varepsilon_i}^2 = \sum_{i=1}^n (y_i - \alpha - \beta_{x_i})^2$

\
Schätzung des Regressionskoeffizienten $\beta$: $\hat{\beta}$

\large
$$\large \hat{\beta} = \frac{Cov(x, y)}{Var(x)} = \frac{\frac{1}{n} \sum_{i=1}^n(x_i - \bar{x})(y_i - \bar{y})} {\frac{1}{n} \sum_{i=1}^n (x_{i} - \bar{x})^2} = \frac{\sum_{i=1}^n(x_i - \bar{x})(y_i - \bar{y})} {\sum_{i=1}^n (x_{i} - \bar{x})^2}$$
$$\large = r_{x, y} \cdot \frac{s_y}{s_x} = \frac{\sum_{i=1}^n(x_i - \bar{x})(y_i - \bar{y})} {\sqrt{\sum_{i=1}^n(x_i - \bar{x})^2 \cdot \sum_{i=1}^n(y_i - \bar{y})^2}} \cdot \frac{\sqrt{\frac{1}{n-1} \sum_{i=1}^n (y_i - \bar{y_i})^2}} {\sqrt{\frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x_i})^2}} $$
\normalsize

Schätzung der Regressionskonstanten $\alpha$: $\hat{\alpha}$

\large
$$\large \hat{\alpha} =  \bar{y} - \hat{\beta}\bar{x}$$
\normalsize

## Determinationskoeffizient $R^2$

- "Erklärungsleistung" eines Regressionsmodells: Anteil erklärter Varianz an Gesamtvarianz
- Maß der Stärke eines Zusammenhangs


## Multiple lineare Regression

Multiple lineare Regressionsgleichung: $y_i = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + … + \beta_kx_{ik} + \varepsilon_i$

- Schätzung der linearen Regression anhand $k$ unabhängiger Variablen für $i=1, \ …, \ n$

\
Matrixschreibweise der Multiplen linearen Regressionsgleichung: 

\large
$$\begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{pmatrix} = 
\begin{pmatrix} x_{11} & x_{12} & \cdots & x_{1k} 
\\ x_{21} & x_{22} & \cdots & x_{2k}
\\ \vdots & \vdots & \ddots & \vdots 
\\ x_{n1} & x_{n2} & \cdots & x_{nk} \end{pmatrix} 
\begin{pmatrix} \beta_1 \\ \beta_2 \\ \vdots \\ \beta_k \end{pmatrix} +
\begin{pmatrix} \varepsilon_1 \\ \varepsilon_2 \\ \vdots \\ \varepsilon_k \end{pmatrix}
$$
$$\mathbf{y = X \beta + \varepsilon}$$
\normalsize

- Schätzung von $\beta$ anhand Kleinste-Quadrate-Schätzung (*OLS*): 

\large
$$\hat{\beta} = \begin{pmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_k \end{pmatrix} = \mathbf{(X'X)^{-1} X'y}$$
\normalsize

- $\mathbf{X'}$: Transponierte Matrix von $\mathbf{X}$
- $\mathbf{(X'X)^{-1}}$: Inverse Matrix von $\mathbf{X'X}$


